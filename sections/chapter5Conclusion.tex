\chapter{Conclusions and Perspectives} \label{chapter5}

\section{Summary}

There has been astonishing progress in the adoption of \ac{AI}/\ac{ML} for population genetics research since we began the works presented in this thesis. This field of research has emerged from prototypical models tailored to relatively bespoke tasks (\cite{sheehan_deep_2016}), gone through speculation and excitement about the promises and pitfalls of a data-driven \ac{ML} approach to evolutionary modeling (\cite{schrider_supervised_2018}), and eventually accumulated a robust body of literature that spans a range of technical and methodological aspects of \ac{ML} tailored to diverse empirical problems in population genetics (\cite{korfmann_deep_2023,huang_harnessing_2023}). My thesis work focuses on utilizing \ac{ML} to fulfill the potential of making accurate inference with complex genealogical information in the \ac{ARG} (Chapters \ref{chapter2} \& \ref{chapter3}) and addressing the fundamental limitation of mis-specified training data for supervised \ac{ML} models (Chapter \ref{chapter4}). This thesis makes a significant contribution to simultaneous efforts in the field that strive to make \ac{AI}/\ac{ML} a powerful and accessible inferential framework for profound evolutionary discoveries.

There are rich opportunities to move forward with this line of work. For example, model interpretability remains a crucial subject for further research in the evolutionary applications of deep learning. Population geneticists are interested in not only predictions, but also mechanistic understanding of evolution. Therefore, despite the superb predictive performance of deep learning models, their ``black-box" nature presents an obstacle to uncovering the evolutionary machinery driving the genetic diversity observed in populations. Much effort has already been devoted to addressing this issue by incorporating the latest techniques from explainable \ac{AI} research. This growing body of work is highlighted in \cite{novakovsky_obtaining_2023}. Below, we conclude the thesis by elaborating on another promising new area that pushes towards a deeper understanding of evolution through \ac{ML}. Generative \ac{AI} has achieve remarkable success in a variety of domains, notably \ac{NLP} and \ac{CV}, but is still in its infancy in population genetics.

\section{Evolutionary modeling in the era of generative \ac{AI}} \label{evo-genAI}

Generative models capture the underlying probability distribution of observed data and consequently are capable of creating novel data points beyond the observed data by sampling from the captured probability distribution. Many traditional population genetic models are generative models, parameterized under theories of evolution. Although fully interpretable, these models often lack either the scalability to handle large amount of modern genomic data or the versatility to accommodate complex evolutionary processes, as discussed previously (section \ref{intro-DL}). Deep generative models provide an alternative of using neural networks to automatically learn the probability distribution from the training data in a domain-agnostic fashion.

%% survey of early generative DL models: RBM, GAN, VAE and introduce existing body of work in popgen
Several early deep generative models have already been applied to population genetic tasks. \Acp{RBM} are energy-based models that map the probability of data to an energy function and have been used to generate artificial genomes mimicking the properties of real ones (\cite{yelmen2021creating,yelmen2023deep}). \Iac{VAE} consists of an encoder that maps the input into a latent space defined by a variational distribution and a decoder that can produce different samples from the distribution. \Acp{VAE} are used to infer population structure and ancestry proportions from large genomic datasets such as the UK Biobank (\cite{meisner2022haplotype}). \Acf{GAN} is another widely popular model architecture and has been applied to infer demography (\cite{wang_automatic_2021}), selection (\cite{riley2023interpreting}), recombination (\cite{gower2023inference}) as well as to generate synthetic genomes (\cite{yelmen2021creating,yelmen2023deep}). \Iac{GAN} contains a generator and a discriminator trained in an adversarial manner, where the discriminator aims to corrected distinguish real data from synthetic data produced by the generator and the generator aims to fool the discriminator by creating realistic synthetic data. These early architectures suffer from various practical issues that limit their applications and have gradually been overshadowed by a new generation of deep generative models (\cite{huang_harnessing_2023}).

The latest and greatest deep generative models are diffusion models (\cite{sohl2015deep}) and transformers (\cite{attention2017_3f5ee243}). Diffusion models typically contain a forward diffusion process where noise is injected at each step and a reverse denoising process where neural networks recover the input by attempting to remove the noise. Diffusion models are most notably used for text-to-image generation and yield impressive results. Transformers pioneered the self-attention mechanism and have achieved unparalleled performance for many \ac{NLP} tasks. In particular, transformers power \acfp{LLM} and help establish a new paradigm where large foundation models such as \acp{LLM} are pre-trained in an unsupervised or self-supervised manner and subsequently fine-tuned to use cases across a wide range of domains.

In light of the striking success of generative \ac{AI} models across many fields, it is timely for applications of \ac{AI}/\ac{ML} in population genetics to move beyond optimization of simple prediction tasks and towards fully generative evolutionary models. Here we introduce two specific avenues of future work among myriad other possibilities in this rising field of research.

As mentioned previously, a great deal of efforts have been devoted to learning evolutionary parameters from large-scale population sequencing data using \acp{GAN}. The fundamental challenge to this approach is that the generator component of the \ac{GAN} is usually a population genetic simulator and therefore non-differentiable. A prototype of this model called “pg-gan” has been trained with a gradient-free method – simulated annealing, which can be computationally prohibitive for high-dimensional search spaces, hence limiting the complexity of the evolutionary model (\cite{wang_automatic_2021}). This problem is reminiscent of non-differentiable criteria encountered in training \acp{LLM} such as human feedback, which have found effective solutions through either reinforcement learning (\cite{christiano2017deep,Hui_2021_CVPR}) or zeroth-order optimizers using gradient approximation techniques such as ZO-SGD (\cite{Spall1992}) and MeZO (\cite{malladi2023fine}). There is great potential in developing \acp{GAN} tailored to complex evolutionary generators where the simulators are trained with either reinforcement learning or zeroth-order optimization algorithms. This new approach has the prospect of accommodating a large set of evolutionary parameters while maintaining computational efficiency.

Pushing the idea of generative models for evolutionary analyses further, an ultimate vision is to make large foundation models of evolution a reality. The emergence of generative \ac{AI} models has made a profound impact on biomedical research. For example, \acp{LLM} of protein and DNA have already shown outstanding performance in a variety of problems in molecular biology such as protein structure (\cite{lin2023evolutionary}) or variant effect prediction (\cite{benegas2023dna,cheng2023accurate}). Similarly, foundation models of evolution based on genealogical embeddings of the \ac{ARG} have the potential to revolutionize population genetic research. In order to build such models, an auto-regressive training procedure for genealogies needs to be developed. One possibility is to borrow the idea of “threading” from ARGweaver (\cite{rasmussen_genome-wide_2014,hubisz_mapping_2020}, see section \ref{intro-arg}) where one left-out sample or subtree is “re-threaded” into the genealogy. We can similarly train a neural network by ``masking" a sample or a subtree and optimizing it to complete the genealogy, akin to masked language modeling for \acp{LLM}. This will pave the way to foundation models pre-trained in a self-supervised manner with an incredibly wide range of simulations of many evolutionary processes. Such models have the potential to “understand” the grammar and logic of how evolutionary histories manifest in different topologies of the \ac{ARG}, much like the way large language models “understand” natural languages. Since access to computational resources is a limiting factor for many academic researchers, the need to train highly parameterized models from scratch remains a hurdle for the adoption of deep learning in population genetics. Foundation models of evolution will create a new paradigm where empiricists can fine-tune high-performing pre-trained models even with limited amount of compute or labeled data instead of creating less powerful models from scratch for each new application to a different population or organism.

Generative \ac{AI} models of evolution will empower rapid scientific discovery that keeps pace with the ever-growing scale of genomic datasets and move the field beyond solving isolated inference problems into a holistic view of evolution.

% promising way to circle back to the original pursuit of 
% After all, to move the field of population genetics forward, we look into the past.