
@article{huang_harnessing_2023,
	title = {Harnessing deep learning for population genetic inference},
	copyright = {2023 Springer Nature Limited},
	issn = {1471-0064},
	url = {https://www.nature.com/articles/s41576-023-00636-3},
	doi = {10.1038/s41576-023-00636-3},
	abstract = {In population genetics, the emergence of large-scale genomic data for various species and populations has provided new opportunities to understand the evolutionary forces that drive genetic diversity using statistical inference. However, the era of population genomics presents new challenges in analysing the massive amounts of genomes and variants. Deep learning has demonstrated state-of-the-art performance for numerous applications involving large-scale data. Recently, deep learning approaches have gained popularity in population genetics; facilitated by the advent of massive genomic data sets, powerful computational hardware and complex deep learning architectures, they have been used to identify population structure, infer demographic history and investigate natural selection. Here, we introduce common deep learning architectures and provide comprehensive guidelines for implementing deep learning models for population genetic inference. We also discuss current challenges and future directions for applying deep learning in population genetics, focusing on efficiency, robustness and interpretability.},
	language = {en},
	urldate = {2023-09-05},
	journal = {Nature Reviews Genetics},
	author = {Huang, Xin and Rymbekova, Aigerim and Dolgova, Olga and Lao, Oscar and Kuhlwilm, Martin},
	month = sep,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Genetic variation, Machine learning},
	pages = {1--18},
}

@article{burger_neural_2022,
	title = {Neural networks for self-adjusting mutation rate estimation when the recombination rate is unknown},
	volume = {18},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010407},
	doi = {10.1371/journal.pcbi.1010407},
	abstract = {Estimating the mutation rate, or equivalently effective population size, is a common task in population genetics. If recombination is low or high, optimal linear estimation methods are known and well understood. For intermediate recombination rates, the calculation of optimal estimators is more challenging. As an alternative to model-based estimation, neural networks and other machine learning tools could help to develop good estimators in these involved scenarios. However, if no benchmark is available it is difficult to assess how well suited these tools are for different applications in population genetics. Here we investigate feedforward neural networks for the estimation of the mutation rate based on the site frequency spectrum and compare their performance with model-based estimators. For this we use the model-based estimators introduced by Fu, Futschik et al., and Watterson that minimize the variance or mean squared error for no and free recombination. We find that neural networks reproduce these estimators if provided with the appropriate features and training sets. Remarkably, using the model-based estimators to adjust the weights of the training data, only one hidden layer is necessary to obtain a single estimator that performs almost as well as model-based estimators for low and high recombination rates, and at the same time provides a superior estimation method for intermediate recombination rates. We apply the method to simulated data based on the human chromosome 2 recombination map, highlighting its robustness in a realistic setting where local recombination rates vary and/or are unknown.},
	language = {en},
	number = {8},
	urldate = {2023-08-08},
	journal = {PLOS Computational Biology},
	author = {Burger, Klara Elisabeth and Pfaffelhuber, Peter and Baumdicker, Franz},
	month = aug,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Adaptive training, Chromosome mapping, Effective population size, Feedforward neural networks, Genomics, Machine learning, Neural networks, Population genetics},
	pages = {e1010407},
}

@article{blum_non-linear_2010,
	title = {Non-linear regression models for {Approximate} {Bayesian} {Computation}},
	volume = {20},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-009-9116-0},
	doi = {10.1007/s11222-009-9116-0},
	abstract = {Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computationally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a nonlinear conditional heteroscedastic regression of the parameter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the computational burden in two examples of inference in statistical genetics and in a queueing model.},
	language = {en},
	number = {1},
	urldate = {2023-08-08},
	journal = {Statistics and Computing},
	author = {Blum, Michael G. B. and Fran√ßois, Olivier},
	month = jan,
	year = {2010},
	keywords = {Approximate Bayesian computation, Coalescent models, Conditional density estimation, Curse of dimensionality, Feed forward neural networks, Heteroscedasticity, Implicit statistical models, Importance sampling, Indirect inference, Likelihood-free inference, Non-linear regression},
	pages = {63--73},
}

@article{johri_recommendations_2022,
	title = {Recommendations for improving statistical inference in population genomics},
	volume = {20},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001669},
	doi = {10.1371/journal.pbio.3001669},
	abstract = {The field of population genomics has grown rapidly in response to the recent advent of affordable, large-scale sequencing technologies. As opposed to the situation during the majority of the 20th century, in which the development of theoretical and statistical population genetic insights outpaced the generation of data to which they could be applied, genomic data are now being produced at a far greater rate than they can be meaningfully analyzed and interpreted. With this wealth of data has come a tendency to focus on fitting specific (and often rather idiosyncratic) models to data, at the expense of a careful exploration of the range of possible underlying evolutionary processes. For example, the approach of directly investigating models of adaptive evolution in each newly sequenced population or species often neglects the fact that a thorough characterization of ubiquitous nonadaptive processes is a prerequisite for accurate inference. We here describe the perils of these tendencies, present our consensus views on current best practices in population genomic data analysis, and highlight areas of statistical inference and theory that are in need of further attention. Thereby, we argue for the importance of defining a biologically relevant baseline model tuned to the details of each new analysis, of skepticism and scrutiny in interpreting model fitting results, and of carefully defining addressable hypotheses and underlying uncertainties.},
	language = {en},
	number = {5},
	urldate = {2023-08-08},
	journal = {PLOS Biology},
	author = {Johri, Parul and Aquadro, Charles F. and Beaumont, Mark and Charlesworth, Brian and Excoffier, Laurent and Eyre-Walker, Adam and Keightley, Peter D. and Lynch, Michael and McVean, Gil and Payseur, Bret A. and Pfeifer, Susanne P. and Stephan, Wolfgang and Jensen, Jeffrey D.},
	month = may,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Deletion mutation, Evolutionary processes, Genome analysis, Genomics, Population genetics, Population size, Statistical data, Statistical distributions},
	pages = {e3001669},
}

@article{raynal_abc_2019,
	title = {{ABC} random forests for {Bayesian} parameter inference},
	volume = {35},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bty867},
	doi = {10.1093/bioinformatics/bty867},
	abstract = {Approximate Bayesian computation (ABC) has grown into a standard methodology that manages Bayesian inference for models associated with intractable likelihood functions. Most ABC implementations require the preliminary selection of a vector of informative statistics summarizing raw data. Furthermore, in almost all existing implementations, the tolerance level that separates acceptance from rejection of simulated parameter values needs to be calibrated.We propose to conduct likelihood-free Bayesian inferences about parameters with no prior selection of the relevant components of the summary statistics and bypassing the derivation of the associated tolerance level. The approach relies on the random forest (RF) methodology of Breiman (2001) applied in a (non-parametric) regression setting. We advocate the derivation of a new RF for each component of the parameter vector of interest. When compared with earlier ABC solutions, this method offers significant gains in terms of robustness to the choice of the summary statistics, does not depend on any type of tolerance level, and is a good trade-off in term of quality of point estimator precision and credible interval estimations for a given computing time. We illustrate the performance of our methodological proposal and compare it with earlier ABC methods on a Normal toy example and a population genetics example dealing with human population evolution.All methods designed here have been incorporated in the R package abcrf (version 1.7.1) available on CRAN.Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2023-08-08},
	journal = {Bioinformatics},
	author = {Raynal, Louis and Marin, Jean-Michel and Pudlo, Pierre and Ribatet, Mathieu and Robert, Christian P and Estoup, Arnaud},
	month = may,
	year = {2019},
	pages = {1720--1728},
}

@article{pudlo_reliable_2016,
	title = {Reliable {ABC} model choice via random forests},
	volume = {32},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btv684},
	doi = {10.1093/bioinformatics/btv684},
	abstract = {Motivation: Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques.Results: We propose a novel approach based on a machine learning tool named random forests (RF) to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with RF and postponing the approximation of the posterior probability of the selected model for a second stage also relying on RF. Compared with earlier implementations of ABC model choice, the ABC RF approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least 50) and (iv) it includes an approximation of the posterior probability of the selected model. The call to RF will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets.Availability and implementation: The proposed methodology is implemented in the R package abcrf available on the CRAN.Contact: ¬†jean-michel.marin@umontpellier.frSupplementary information: ¬†Supplementary data are available at Bioinformatics online.},
	number = {6},
	urldate = {2023-08-08},
	journal = {Bioinformatics},
	author = {Pudlo, Pierre and Marin, Jean-Michel and Estoup, Arnaud and Cornuet, Jean-Marie and Gautier, Mathieu and Robert, Christian P.},
	month = mar,
	year = {2016},
	pages = {859--866},
}

@misc{lauterbur_expanding_2022,
	title = {Expanding the stdpopsim species catalog, and lessons learned for realistic genome simulations},
	copyright = {¬© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.29.514266v1},
	doi = {10.1101/2022.10.29.514266},
	abstract = {Simulation is a key tool in population genetics for both methods development and empirical research, but producing simulations that recapitulate the main features of genomic data sets remains a major obstacle. Today, more realistic simulations are possible thanks to large increases in the quantity and quality of available genetic data, and to the sophistication of inference and simulation software. However, implementing these simulations still requires substantial time and specialized knowledge. These challenges are especially pronounced for simulating genomes for species that are not well-studied, since it is not always clear what information is required to produce simulations with a level of realism sufficient to confidently answer a given question. The community-developed framework stdpopsim seeks to lower this barrier by facilitating the simulation of complex population genetic models using up-to-date information. The initial version of stdpopsim focused on establishing this framework using six well-characterized model species (Adrion et al.,2020). Here, we report on major improvements made in the new release of stdpopsim (version 0.2), which includes a significant expansion of the species catalog and substantial additions to simulation capabilities. Features added to improve the realism of the simulated genomes include non-crossover recombination and provision of species-specific genomic annotations. Through community-driven efforts, we expanded the number of species in the catalog more than three-fold and broadened coverage across the tree of life. During the process of expanding the catalog, we have identified common sticking points and developed best practices for setting up genome-scale simulations. We describe the input data required for generating a realistic simulation, suggest good practices for obtaining the relevant information from the literature, and discuss common pitfalls and major considerations. These improvements to stdpopsim aim to further promote the use of realistic whole-genome population genetic simulations, especially in non-model organisms, making them available, transparent, and accessible to everyone.},
	language = {en},
	urldate = {2023-03-15},
	publisher = {bioRxiv},
	author = {Lauterbur, M. Elise and Cavassim, Maria Izabel A. and Gladstein, Ariella L. and Gower, Graham and Pope, Nathaniel S. and Tsambos, Georgia and Adrion, Jeff and Belsare, Saurabh and Biddanda, Arjun and Caudill, Victoria and Cury, Jean and Echevarria, Ignacio and Haller, Benjamin C. and Hasan, Ahmed R. and Huang, Xin and Iasi, Leonardo Nicola Martin and Noskova, Ekaterina and Ob≈°teter, Jana and Pavinato, Vitor Antonio Corr√™a and Pearson, Alice and Peede, David and Perez, Manolo F. and Rodrigues, Murillo F. and Smith, Chris C. R. and Spence, Jeffrey P. and Teterina, Anastasia and Tittes, Silas and Unneberg, Per and Vazquez, Juan Manuel and Waples, Ryan K. and Wohns, Anthony Wilder and Wong, Yan and Baumdicker, Franz and Cartwright, Reed A. and Gorjanc, Gregor and Gutenkunst, Ryan N. and Kelleher, Jerome and Kern, Andrew D. and Ragsdale, Aaron P. and Ralph, Peter L. and Schrider, Daniel R. and Gronau, Ilan},
	month = oct,
	year = {2022},
	note = {Pages: 2022.10.29.514266
Section: New Results},
}

@article{adrion_community-maintained_2020,
	title = {A community-maintained standard library of population genetic models},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.54967},
	doi = {10.7554/eLife.54967},
	abstract = {The explosion in population genomic data demands ever more complex modes of analysis, and increasingly, these analyses depend on sophisticated simulations. Recent advances in population genetic simulation have made it possible to simulate large and complex models, but specifying such models for a particular simulation engine remains a difficult and error-prone task. Computational genetics researchers currently re-implement simulation models independently, leading to inconsistency and duplication of effort. This situation presents a major barrier to empirical researchers seeking to use simulations for power analyses of upcoming studies or sanity checks on existing genomic data. Population genetics, as a field, also lacks standard benchmarks by which new tools for inference might be measured. Here, we describe a new resource, stdpopsim, that attempts to rectify this situation. Stdpopsim is a community-driven open source project, which provides easy access to a growing catalog of published simulation models from a range of organisms and supports multiple simulation engine backends. This resource is available as a well-documented python library with a simple command-line interface. We share some examples demonstrating how stdpopsim can be used to systematically compare demographic inference methods, and we encourage a broader community of developers to contribute to this growing resource.},
	urldate = {2023-03-15},
	journal = {eLife},
	author = {Adrion, Jeffrey R and Cole, Christopher B and Dukler, Noah and Galloway, Jared G and Gladstein, Ariella L and Gower, Graham and Kyriazis, Christopher C and Ragsdale, Aaron P and Tsambos, Georgia and Baumdicker, Franz and Carlson, Jedidiah and Cartwright, Reed A and Durvasula, Arun and Gronau, Ilan and Kim, Bernard Y and McKenzie, Patrick and Messer, Philipp W and Noskova, Ekaterina and Ortega-Del Vecchyo, Diego and Racimo, Fernando and Struck, Travis J and Gravel, Simon and Gutenkunst, Ryan N and Lohmueller, Kirk E and Ralph, Peter L and Schrider, Daniel R and Siepel, Adam and Kelleher, Jerome and Kern, Andrew D},
	editor = {Coop, Graham and Wittkopp, Patricia J and Novembre, John and Sethuraman, Arun and Mathieson, Sara},
	month = jun,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {open source, reproducibility, simulation},
	pages = {e54967},
}

@misc{papers_with_code_domain_nodate,
	title = {Domain {Adaptation}},
	url = {https://paperswithcode.com/task/domain-adaptation},
	abstract = {Domain adaptation is the task of adapting models across domains. This is motivated by the challenge where the test and training datasets fall from different data distributions due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions. 

Further readings:

- [A Brief Review of Domain Adaptation](https://paperswithcode.com/paper/a-brief-review-of-domain-adaptation)

{\textless}span style="color:grey; opacity: 0.6"{\textgreater}( Image credit: [Unsupervised Image-to-Image Translation Networks](https://arxiv.org/pdf/1703.00848v6.pdf) ){\textless}/span{\textgreater}},
	language = {en},
	urldate = {2023-03-01},
	author = {Papers with Code},
}

@article{korfmann_deep_2023,
	title = {Deep {Learning} in {Population} {Genetics}},
	volume = {15},
	issn = {1759-6653},
	url = {https://doi.org/10.1093/gbe/evad008},
	doi = {10.1093/gbe/evad008},
	abstract = {Population genetics is transitioning into a data-driven discipline thanks to the availability of large-scale genomic data and the need to study increasingly complex evolutionary scenarios. With likelihood and Bayesian approaches becoming either intractable or computationally unfeasible, machine learning, and in particular deep learning, algorithms are emerging as popular techniques for population genetic inferences. These approaches rely on algorithms that learn non-linear relationships between the input data and the model parameters being estimated through representation learning from training data sets. Deep learning algorithms currently employed in the field comprise discriminative and generative models with fully connected, convolutional, or recurrent layers. Additionally, a wide range of powerful simulators to generate training data under complex scenarios are now available. The application of deep learning to empirical data sets mostly replicates previous findings of demography reconstruction and signals of natural selection in model organisms. To showcase the feasibility of deep learning to tackle new challenges, we designed a branched architecture to detect signals of recent balancing selection from temporal haplotypic data, which exhibited good predictive performance on simulated data. Investigations on the interpretability of neural networks, their robustness to uncertain training data, and creative representation of population genetic data, will provide further opportunities for technological advancements in the field.},
	number = {2},
	urldate = {2023-02-07},
	journal = {Genome Biology and Evolution},
	author = {Korfmann, Kevin and Gaggiotti, Oscar E and Fumagalli, Matteo},
	month = feb,
	year = {2023},
	pages = {evad008},
}

@article{kong_fine-scale_2010,
	title = {Fine-scale recombination rate differences between sexes, populations and individuals},
	volume = {467},
	copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature09525},
	doi = {10.1038/nature09525},
	abstract = {High-resolution recombination maps serve many purposes in genetic research. The currently available maps, which use linkage disequilibrium patterns of high-density SNP (single nucleotide polymorphism) data from the HapMap project, have proved to be very useful. But they have some limitations; for instance, they do not provide information on differences in recombination characteristics between and within the sexes. A team at biopharmaceutical firm deCODE genetics in Reykjavik has used genome-wide SNP data from more than 15,000 parent‚Äìoffspring pairs to construct the first recombination maps based on directly observed recombination events, providing resolution down to 10 kilobases. Their data reveal interesting recombination differences between the sexes. In males, for example, recombination tends to shuffle exons, whereas in females it generates new combinations of nearby genes. Comparisons of these maps with those based on linkage disequilibrium reveal previously unrecognized differences between populations in Europe, Africa and the United States.},
	language = {en},
	number = {7319},
	urldate = {2022-12-01},
	journal = {Nature},
	author = {Kong, Augustine and Thorleifsson, Gudmar and Gudbjartsson, Daniel F. and Masson, Gisli and Sigurdsson, Asgeir and Jonasdottir, Aslaug and Walters, G. Bragi and Jonasdottir, Adalbjorg and Gylfason, Arnaldur and Kristinsson, Kari Th and Gudjonsson, Sigurjon A. and Frigge, Michael L. and Helgason, Agnar and Thorsteinsdottir, Unnur and Stefansson, Kari},
	month = oct,
	year = {2010},
	note = {Number: 7319
Publisher: Nature Publishing Group},
	keywords = {Anthropology, DNA recombination, Genetic variation},
	pages = {1099--1103},
}

@inproceedings{ghifary_deep_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Reconstruction}-{Classification} {Networks} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_36},
	abstract = {In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: (i) supervised classification of labeled source data, and (ii) unsupervised reconstruction of unlabeled target data. In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks.},
	language = {en},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie and Balduzzi, David and Li, Wen},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional networks, Deep learning, Domain adaptation, Object recognition, Transfer learning},
	pages = {597--613},
}

@inproceedings{liu_coupled_2016,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/502e4a16930e414107ee22b6198c578f-Abstract.html},
	abstract = {We propose the coupled generative adversarial nets (CoGAN) framework for generating pairs of corresponding images in two different domains. The framework consists of a pair of generative adversarial nets, each responsible for generating images in one domain. We show that by enforcing a simple weight-sharing constraint, the CoGAN learns to generate pairs of corresponding images without existence of any pairs of corresponding images in the two domains in the training set. In other words, the CoGAN learns a joint distribution of images in the two domains from images drawn separately from the marginal distributions of the individual domains. This is in contrast to the existing multi-modal generative models, which require corresponding images for training. We apply the CoGAN to several pair image generation tasks. For each task, the CoGAN learns to generate convincing pairs of corresponding images. We further demonstrate the applications of the CoGAN framework for the domain adaptation and cross-domain image generation tasks.},
	urldate = {2022-10-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	year = {2016},
}

@article{rozantsev_beyond_2019,
	title = {Beyond {Sharing} {Weights} for {Deep} {Domain} {Adaptation}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2814042},
	abstract = {The performance of a classifier trained on data coming from a specific domain typically degrades when applied to a related but different one. While annotating many samples from the new domain would address this issue, it is often too expensive or impractical. Domain Adaptation has therefore emerged as a solution to this problem; It leverages annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which it is either sparse or even lacking altogether. In this context, the recent trend consists of learning deep architectures whose weights are shared for both domains, which essentially amounts to learning domain invariant features. Here, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, where one operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rozantsev, Artem and Salzmann, Mathieu and Fua, Pascal},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computer architecture, Computer vision, Detectors, Domain adaptation, Machine learning, Task analysis, Training, Training data, deep learning},
	pages = {801--814},
}

@incollection{csurka_comprehensive_2017,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {A {Comprehensive} {Survey} on {Domain} {Adaptation} for {Visual} {Applications}},
	isbn = {978-3-319-58347-1},
	url = {https://doi.org/10.1007/978-3-319-58347-1_1},
	abstract = {The aim of this chapter is to give an overview of domain adaptation and transfer learning with a specific view to visual applications. After a general motivation, we first position domain adaptation in the more general transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and heterogeneous domain adaptationDomain Adaptation (DA)heterogeneous domain adaptation, heterogeneous DA, (HDA)methods. Third, we discuss the effect of the success of deep convolutional architectures which led to the new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we review DA methods that go beyond image categorization, such as object detection, image segmentation, video analyses or learning visual attributes. We conclude the chapter with a section where we relate domain adaptation to other machine learning solutions.},
	language = {en},
	urldate = {2022-10-28},
	booktitle = {Domain {Adaptation} in {Computer} {Vision} {Applications}},
	publisher = {Springer International Publishing},
	author = {Csurka, Gabriela},
	editor = {Csurka, Gabriela},
	year = {2017},
	doi = {10.1007/978-3-319-58347-1_1},
	pages = {1--35},
}

@article{pan_domain_2011,
	title = {Domain {Adaptation} via {Transfer} {Component} {Analysis}},
	volume = {22},
	issn = {1941-0093},
	doi = {10.1109/TNN.2010.2091281},
	abstract = {Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
	month = feb,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Dimensionality reduction, Feature extraction, Hilbert space, Hilbert space embedding of distributions, Kernel, Learning systems, Manifolds, Noise measurement, Optimization, domain adaptation, transfer learning},
	pages = {199--210},
}

@inproceedings{sun_return_2016,
	address = {Phoenix, Arizona},
	series = {{AAAI}'16},
	title = {Return of frustratingly easy domain adaptation},
	abstract = {Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple‚Äìit can be implemented in four lines of Matlab code‚ÄìCORAL performs remarkably well in extensive evaluations on standard benchmark datasets.},
	urldate = {2022-10-28},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
	month = feb,
	year = {2016},
	pages = {2058--2065},
}

@inproceedings{fernando_unsupervised_2013,
	title = {Unsupervised {Visual} {Domain} {Adaptation} {Using} {Subspace} {Alignment}},
	doi = {10.1109/ICCV.2013.368},
	abstract = {In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyper parameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},
	month = dec,
	year = {2013},
	note = {ISSN: 2380-7504},
	keywords = {Context, Covariance matrices, Eigenvalues and eigenfunctions, Manifolds, Principal component analysis, Support vector machines, Vectors, domain adaptation, object recognition, subspace alignment},
	pages = {2960--2967},
}

@misc{daume_iii_frustratingly_2009,
	title = {Frustratingly {Easy} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/0907.1815},
	doi = {10.48550/arXiv.0907.1815},
	abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Daum√© III, Hal},
	month = jul,
	year = {2009},
	note = {arXiv:0907.1815 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{dai_boosting_2007,
	address = {New York, NY, USA},
	series = {{ICML} '07},
	title = {Boosting for transfer learning},
	isbn = {978-1-59593-793-3},
	url = {https://doi.org/10.1145/1273496.1273521},
	doi = {10.1145/1273496.1273521},
	abstract = {Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund \& Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.},
	urldate = {2022-10-28},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
	month = jun,
	year = {2007},
	pages = {193--200},
}

@article{shimodaira_improving_2000,
	title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
	volume = {90},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	doi = {10.1016/S0378-3758(00)00115-4},
	abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is effective in cases such as sample surveys or design of experiments, where the observed covariate follows a different distribution than that in the whole population. Under misspecification of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is defined by the expected Kullback‚ÄìLeibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct specification of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.},
	language = {en},
	number = {2},
	urldate = {2022-10-28},
	journal = {Journal of Statistical Planning and Inference},
	author = {Shimodaira, Hidetoshi},
	month = oct,
	year = {2000},
	keywords = {Akaike information criterion, Design of experiments, Importance sampling, Kullback‚ÄìLeibler divergence, Misspecification, Sample surveys, Weighted least squares},
	pages = {227--244},
}

@article{boyko_assessing_2008,
	title = {Assessing the {Evolutionary} {Impact} of {Amino} {Acid} {Mutations} in the {Human} {Genome}},
	volume = {4},
	issn = {1553-7404},
	url = {https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000083},
	doi = {10.1371/journal.pgen.1000083},
	abstract = {Quantifying the distribution of fitness effects among newly arising mutations in the human genome is key to resolving important debates in medical and evolutionary genetics. Here, we present a method for inferring this distribution using Single Nucleotide Polymorphism (SNP) data from a population with non-stationary demographic history (such as that of modern humans). Application of our method to 47,576 coding SNPs found by direct resequencing of 11,404 protein coding-genes in 35 individuals (20 European Americans and 15 African Americans) allows us to assess the relative contribution of demographic and selective effects to patterning amino acid variation in the human genome. We find evidence of an ancient population expansion in the sample with African ancestry and a relatively recent bottleneck in the sample with European ancestry. After accounting for these demographic effects, we find strong evidence for great variability in the selective effects of new amino acid replacing mutations. In both populations, the patterns of variation are consistent with a leptokurtic distribution of selection coefficients (e.g., gamma or log-normal) peaked near neutrality. Specifically, we predict 27‚Äì29\% of amino acid changing (nonsynonymous) mutations are neutral or nearly neutral ({\textbar}s{\textbar}{\textless}0.01\%), 30‚Äì42\% are moderately deleterious (0.01\%{\textless}{\textbar}s{\textbar}{\textless}1\%), and nearly all the remainder are highly deleterious or lethal ({\textbar}s{\textbar}{\textgreater}1\%). Our results are consistent with 10‚Äì20\% of amino acid differences between humans and chimpanzees having been fixed by positive selection with the remainder of differences being neutral or nearly neutral. Our analysis also predicts that many of the alleles identified via whole-genome association mapping may be selectively neutral or (formerly) positively selected, implying that deleterious genetic variation affecting disease phenotype may be missed by this widely used approach for mapping genes underlying complex traits.},
	language = {en},
	number = {5},
	urldate = {2022-10-28},
	journal = {PLOS Genetics},
	author = {Boyko, Adam R. and Williamson, Scott H. and Indap, Amit R. and Degenhardt, Jeremiah D. and Hernandez, Ryan D. and Lohmueller, Kirk E. and Adams, Mark D. and Schmidt, Steffen and Sninsky, John J. and Sunyaev, Shamil R. and White, Thomas J. and Nielsen, Rasmus and Clark, Andrew G. and Bustamante, Carlos D.},
	month = may,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {African American people, Deletion mutation, Europe, Gamma spectrometry, Human genomics, Mutation, Natural selection, Single nucleotide polymorphisms},
	pages = {e1000083},
}

@inproceedings{nguyen-meidine_unsupervised_2021,
	title = {Unsupervised {Multi}-{Target} {Domain} {Adaptation} {Through} {Knowledge} {Distillation}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Le_Thanh_Nguyen-Meidine_Unsupervised_Multi-Target_Domain_Adaptation_Through_Knowledge_Distillation_WACV_2021_paper.html},
	language = {en},
	urldate = {2022-10-27},
	author = {Nguyen-Meidine, Le Thanh and Belal, Atif and Kiran, Madhu and Dolz, Jose and Blais-Morin, Louis-Antoine and Granger, Eric},
	year = {2021},
	pages = {1339--1347},
}

@inproceedings{isobe_multi-target_2021,
	title = {Multi-{Target} {Domain} {Adaptation} {With} {Collaborative} {Consistency} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Isobe_Multi-Target_Domain_Adaptation_With_Collaborative_Consistency_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-10-27},
	author = {Isobe, Takashi and Jia, Xu and Chen, Shuaijun and He, Jianzhong and Shi, Yongjie and Liu, Jianzhuang and Lu, Huchuan and Wang, Shengjin},
	year = {2021},
	pages = {8187--8196},
}

@inproceedings{roy_curriculum_2021,
	title = {Curriculum {Graph} {Co}-{Teaching} for {Multi}-{Target} {Domain} {Adaptation}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Roy_Curriculum_Graph_Co-Teaching_for_Multi-Target_Domain_Adaptation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-10-27},
	author = {Roy, Subhankar and Krivosheev, Evgeny and Zhong, Zhun and Sebe, Nicu and Ricci, Elisa},
	year = {2021},
	pages = {5351--5360},
}

@misc{caldas_inference_2022,
	title = {Inference of selective sweep parameters through supervised learning},
	copyright = {¬© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.19.500702v1},
	doi = {10.1101/2022.07.19.500702},
	abstract = {A selective sweep occurs when positive selection drives an initially rare allele to high population frequency. In nature, the precise parameters of a sweep are seldom known: How strong was positive selection? Did the sweep involve only a single adaptive allele (hard sweep) or were multiple adaptive alleles at the locus sweeping at the same time (soft sweep)? If the sweep was soft, did these alleles originate from recurrent new mutations (RNM) or from standing genetic variation (SGV)? Here, we present a method based on supervised machine learning to infer such parameters from the patterns of genetic variation observed around a given sweep locus. Our method is trained on sweep data simulated with SLiM, a fast and flexible framework that allows us to generate training data across a wide spectrum of evolutionary scenarios and can be tailored towards the specific population of interest. Inferences are based on summary statistics describing patterns of nucleotide diversity, haplotype structure, and linkage disequilibrium, which are estimated across systematically varying genomic window sizes to capture sweeps across a wide range of selection strengths. We show that our method can accurately infer selection coefficients in the range 0.01 {\textless} s {\textless} 100 and classify sweep types between hard sweeps, RNM soft sweeps, and SGV soft sweeps with accuracy 69 \% to 95 \% depending on sweep strength. We also show that the method infers the correct sweep types at three empirical loci known to be associated with the recent evolution of pesticide resistance in Drosophila melanogaster. Our study demonstrates the power of machine learning for inferring sweep parameters from present-day genotyping samples, opening the door to a better understanding of the modes of adaptive evolution in nature.
Author summary Adaptation often involves the rapid spread of a beneficial genetic variant through the population in a process called a selective sweep. Here, we develop a method based on machine learning that can infer the strength of selection driving such a sweep, and distinguish whether it involved only a single adaptive variant (a so-called hard sweep) or several adaptive variants of independent origin that were simultaneously rising in frequency at the same genomic position (a so-called soft selective sweep). Our machine learning method is trained on simulated data and only requires data sampled from a single population at a single point in time. To address the challenge of simulating realistic datasets for training, we explore the behavior of the method under a variety of testing scenarios, including scenarios where the history of the population of interest was misspecified. Finally, to illustrate the accuracy of our method, we apply it to three known sweep loci that have contributed to the evolution of pesticide resistance in Drosophila melanogaster.},
	language = {en},
	urldate = {2022-08-09},
	publisher = {bioRxiv},
	author = {Caldas, Ian V. and Clark, Andrew G. and Messer, Philipp W.},
	month = jul,
	year = {2022},
	note = {Pages: 2022.07.19.500702
Section: New Results},
}

@article{speidel_method_2019,
	title = {A method for genome-wide genealogy estimation for thousands of samples},
	volume = {51},
	issn = {1061-4036, 1546-1718},
	url = {http://www.nature.com/articles/s41588-019-0484-x},
	doi = {10.1038/s41588-019-0484-x},
	language = {en},
	number = {9},
	urldate = {2020-06-11},
	journal = {Nature Genetics},
	author = {Speidel, Leo and Forest, Marie and Shi, Sinan and Myers, Simon R.},
	month = sep,
	year = {2019},
	pages = {1321--1329},
}

@article{kim_distance_2020,
	title = {Distance metrics for ranked evolutionary trees},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1922851117},
	doi = {10.1073/pnas.1922851117},
	number = {46},
	urldate = {2022-07-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kim, Jaehee and Rosenberg, Noah A. and Palacios, Julia A.},
	month = nov,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {28876--28886},
}

@article{gronau_bayesian_2011,
	title = {Bayesian inference of ancient human demography from individual genome sequences},
	volume = {43},
	copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/ng.937},
	doi = {10.1038/ng.937},
	abstract = {Adam Siepel and colleagues estimate key parameters for ancient human demography using a Bayesian analysis of the whole-genome sequences of six individuals from diverse populations. They present new methods for coalescent-based inference of demographic parameters as well as a custom pipeline for genotype inference.},
	language = {en},
	number = {10},
	urldate = {2022-07-26},
	journal = {Nature Genetics},
	author = {Gronau, Ilan and Hubisz, Melissa J. and Gulko, Brad and Danko, Charles G. and Siepel, Adam},
	month = oct,
	year = {2011},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Evolutionary biology, Genomics},
	pages = {1031--1034},
}

@article{cochran_domain-adaptive_2022,
	title = {Domain-adaptive neural networks improve cross-species prediction of transcription factor binding},
	volume = {32},
	issn = {1088-9051, 1549-5469},
	url = {https://genome.cshlp.org/content/32/3/512},
	doi = {10.1101/gr.275394.121},
	abstract = {The intrinsic DNA sequence preferences and cell type‚Äìspecific cooperative partners of transcription factors (TFs) are typically highly conserved. Hence, despite the rapid evolutionary turnover of individual TF binding sites, predictive sequence models of cell type‚Äìspecific genomic occupancy of a TF in one species should generalize to closely matched cell types in a related species. To assess the viability of cross-species TF binding prediction, we train neural networks to discriminate ChIP-seq peak locations from genomic background and evaluate their performance within and across species. Cross-species predictive performance is consistently worse than within-species performance, which we show is caused in part by species-specific repeats. To account for this domain shift, we use an augmented network architecture to automatically discourage learning of training species‚Äìspecific sequence features. This domain adaptation approach corrects for prediction errors on species-specific repeats and improves overall cross-species model performance. Our results show that cross-species TF binding prediction is feasible when models account for domain shifts driven by species-specific repeats.},
	language = {en},
	number = {3},
	urldate = {2022-07-25},
	journal = {Genome Research},
	author = {Cochran, Kelly and Srivastava, Divyanshi and Shrikumar, Avanti and Balsubramani, Akshay and Hardison, Ross C. and Kundaje, Anshul and Mahony, Shaun},
	month = mar,
	year = {2022},
	pmid = {35042722},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	pages = {512--523},
}

@article{ganin_unsupervised_2014,
	title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
	url = {https://arxiv.org/abs/1409.7495v2},
	doi = {10.48550/arXiv.1409.7495},
	abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
	language = {en},
	urldate = {2022-07-20},
	author = {Ganin, Yaroslav and Lempitsky, Victor},
	month = sep,
	year = {2014},
}

@article{wilson_survey_2020,
	title = {A {Survey} of {Unsupervised} {Deep} {Domain} {Adaptation}},
	volume = {11},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3400066},
	doi = {10.1145/3400066},
	abstract = {Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.},
	number = {5},
	urldate = {2022-07-18},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Wilson, Garrett and Cook, Diane J.},
	month = jul,
	year = {2020},
	keywords = {Domain adaptation, deep learning, generative adversarial networks},
	pages = {51:1--51:46},
}

@article{wang_automatic_2021,
	title = {Automatic inference of demographic parameters using generative adversarial networks},
	volume = {21},
	issn = {1755-0998},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1755-0998.13386},
	doi = {10.1111/1755-0998.13386},
	abstract = {Population genetics relies heavily on simulated data for validation, inference and intuition. In particular, since the evolutionary ‚Äòground truth‚Äô for real data is always limited, simulated data are crucial for training supervised machine learning methods. Simulation software can accurately model evolutionary processes but requires many hand-selected input parameters. As a result, simulated data often fail to mirror the properties of real genetic data, which limits the scope of methods that rely on it. Here, we develop a novel approach to estimating parameters in population genetic models that automatically adapts to data from any population. Our method, pg-gan, is based on a generative adversarial network that gradually learns to generate realistic synthetic data. We demonstrate that our method is able to recover input parameters in a simulated isolation-with-migration model. We then apply our method to human data from the 1000 Genomes Project and show that we can accurately recapitulate the features of real data.},
	language = {en},
	number = {8},
	urldate = {2022-07-18},
	journal = {Molecular Ecology Resources},
	author = {Wang, Zhanpeng and Wang, Jiaping and Kourakos, Michael and Hoang, Nhung and Lee, Hyong Hark and Mathieson, Iain and Mathieson, Sara},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.13386},
	keywords = {demographic inference, evolutionary modelling, generative adversarial network, simulated data},
	pages = {2689--2705},
}

@article{tennessen_evolution_2012,
	title = {Evolution and {Functional} {Impact} of {Rare} {Coding} {Variation} from {Deep} {Sequencing} of {Human} {Exomes}},
	volume = {337},
	url = {https://www.science.org/doi/10.1126/science.1219240},
	doi = {10.1126/science.1219240},
	number = {6090},
	urldate = {2022-07-17},
	journal = {Science},
	author = {Tennessen, Jacob A. and Bigham, Abigail W. and O‚ÄôConnor, Timothy D. and Fu, Wenqing and Kenny, Eimear E. and Gravel, Simon and McGee, Sean and Do, Ron and Liu, Xiaoming and Jun, Goo and Kang, Hyun Min and Jordan, Daniel and Leal, Suzanne M. and Gabriel, Stacey and Rieder, Mark J. and Abecasis, Goncalo and Altshuler, David and Nickerson, Deborah A. and Boerwinkle, Eric and Sunyaev, Shamil and Bustamante, Carlos D. and Bamshad, Michael J. and Akey, Joshua M. and {Broad GO} and {Seattle GO} and {ON BEHALF OF THE NHLBI EXOME SEQUENCING PROJECT}},
	month = jul,
	year = {2012},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {64--69},
}

@article{baumdicker_efficient_2022,
	title = {Efficient ancestry and mutation simulation with msprime 1.0},
	volume = {220},
	issn = {1943-2631},
	url = {https://doi.org/10.1093/genetics/iyab229},
	doi = {10.1093/genetics/iyab229},
	abstract = {Stochastic simulation is a key tool in population genetics, since the models involved are often analytically intractable and simulation is usually the only way of obtaining ground-truth data to evaluate inferences. Because of this, a large number of specialized simulation programs have been developed, each filling a particular niche, but with largely overlapping functionality and a substantial duplication of effort. Here, we introduce msprime version 1.0, which efficiently implements ancestry and mutation simulations based on the succinct tree sequence data structure and the tskit library. We summarize msprime‚Äôs many features, and show that its performance is excellent, often many times faster and more memory efficient than specialized alternatives. These high-performance features have been thoroughly tested and validated, and built using a collaborative, open source development model, which reduces duplication of effort and promotes software quality via community engagement.},
	number = {3},
	urldate = {2022-07-17},
	journal = {Genetics},
	author = {Baumdicker, Franz and Bisschop, Gertjan and Goldstein, Daniel and Gower, Graham and Ragsdale, Aaron P and Tsambos, Georgia and Zhu, Sha and Eldon, Bjarki and Ellerman, E Castedo and Galloway, Jared G and Gladstein, Ariella L and Gorjanc, Gregor and Guo, Bing and Jeffery, Ben and Kretzschumar, Warren W and Lohse, Konrad and Matschiner, Michael and Nelson, Dominic and Pope, Nathaniel S and Quinto-Cort√©s, Consuelo D and Rodrigues, Murillo F and Saunack, Kumar and Sellinger, Thibaut and Thornton, Kevin and van Kemenade, Hugo and Wohns, Anthony W and Wong, Yan and Gravel, Simon and Kern, Andrew D and Koskela, Jere and Ralph, Peter L and Kelleher, Jerome},
	month = mar,
	year = {2022},
	pages = {iyab229},
}

@article{haller_slim_2019,
	title = {{SLiM} 3: {Forward} {Genetic} {Simulations} {Beyond} the {Wright}‚Äì{Fisher} {Model}},
	volume = {36},
	issn = {0737-4038},
	shorttitle = {{SLiM} 3},
	url = {https://doi.org/10.1093/molbev/msy228},
	doi = {10.1093/molbev/msy228},
	abstract = {With the desire to model population genetic processes under increasingly realistic scenarios, forward genetic simulations have become a critical part of the toolbox of modern evolutionary biology. The SLiM forward genetic simulation framework is one of the most powerful and widely used tools in this area. However, its foundation in the Wright‚ÄìFisher model has been found to pose an obstacle to implementing many types of models; it is difficult to adapt the Wright‚ÄìFisher model, with its many assumptions, to modeling ecologically realistic scenarios such as explicit space, overlapping generations, individual variation in reproduction, density-dependent population regulation, individual variation in dispersal or migration, local extinction and recolonization, mating between subpopulations, age structure, fitness-based survival and hard selection, emergent sex ratios, and so forth. In response to this need, we here introduce SLiM 3, which contains two key advancements aimed at abolishing these limitations. First, the new non-Wright‚ÄìFisher or ‚ÄúnonWF‚Äù model type provides a much more flexible foundation that allows the easy implementation of all of the above scenarios and many more. Second, SLiM 3 adds support for continuous space, including spatial interactions and spatial maps of environmental variables. We provide a conceptual overview of these new features, and present several example models to illustrate their use.},
	number = {3},
	urldate = {2022-07-17},
	journal = {Molecular Biology and Evolution},
	author = {Haller, Benjamin C and Messer, Philipp W},
	month = mar,
	year = {2019},
	pages = {632--637},
}

@article{haller_tree-sequence_2019,
	title = {Tree-sequence recording in {SLiM} opens new horizons for forward-time simulation of whole genomes},
	volume = {19},
	issn = {1755-0998},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1755-0998.12968},
	doi = {10.1111/1755-0998.12968},
	abstract = {There is an increasing demand for evolutionary models to incorporate relatively realistic dynamics, ranging from selection at many genomic sites to complex demography, population structure, and ecological interactions. Such models can generally be implemented as individual-based forward simulations, but the large computational overhead of these models often makes simulation of whole chromosome sequences in large populations infeasible. This situation presents an important obstacle to the field that requires conceptual advances to overcome. The recently developed tree-sequence recording method (Kelleher, Thornton, Ashander, \& Ralph, 2018), which stores the genealogical history of all genomes in the simulated population, could provide such an advance. This method has several benefits: (1) it allows neutral mutations to be omitted entirely from forward-time simulations and added later, thereby dramatically improving computational efficiency; (2) it allows neutral burn-in to be constructed extremely efficiently after the fact, using ‚Äúrecapitation‚Äù; (3) it allows direct examination and analysis of the genealogical trees along the genome; and (4) it provides a compact representation of a population's genealogy that can be analysed in Python using the msprime package. We have implemented the tree-sequence recording method in SLiM 3 (a free, open-source evolutionary simulation software package) and extended it to allow the recording of non-neutral mutations, greatly broadening the utility of this method. To demonstrate the versatility and performance of this approach, we showcase several practical applications that would have been beyond the reach of previously existing methods, opening up new horizons for the modelling and exploration of evolutionary processes.},
	language = {en},
	number = {2},
	urldate = {2022-07-17},
	journal = {Molecular Ecology Resources},
	author = {Haller, Benjamin C. and Galloway, Jared and Kelleher, Jerome and Messer, Philipp W. and Ralph, Peter L.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.12968},
	keywords = {background selection, coalescent, genealogical history, pedigree recording, selective sweeps, tree sequences},
	pages = {552--566},
}

@article{abadi_tensorflow_nodate,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	language = {en},
	author = {Abadi, Martƒ±n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	pages = {19},
}

@article{hejase_deep-learning_2022,
	title = {A {Deep}-{Learning} {Approach} for {Inference} of {Selective} {Sweeps} from the {Ancestral} {Recombination} {Graph}},
	volume = {39},
	issn = {1537-1719},
	url = {https://doi.org/10.1093/molbev/msab332},
	doi = {10.1093/molbev/msab332},
	abstract = {Detecting signals of selection from genomic data is a central problem in population genetics. Coupling the rich information in the ancestral recombination graph (ARG) with a powerful and scalable deep-learning framework, we developed a novel method to detect and quantify positive selection: Selection Inference using the Ancestral recombination graph (SIA). Built on a Long Short-Term Memory (LSTM) architecture, a particular type of a Recurrent Neural Network (RNN), SIA can be trained to explicitly infer a full range of selection coefficients, as well as the allele frequency trajectory and time of selection onset. We benchmarked SIA extensively on simulations under a European human demographic model, and found that it performs as well or better as some of the best available methods, including state-of-the-art machine-learning and ARG-based methods. In addition, we used SIA to estimate selection coefficients at several loci associated with human phenotypes of interest. SIA detected novel signals of selection particular to the European (CEU) population at the MC1R and ABCC11 loci. In addition, it recapitulated signals of selection at the LCT locus and several pigmentation-related genes. Finally, we reanalyzed polymorphism data of a collection of recently radiated southern capuchino seedeater taxa in the genus Sporophila to quantify the strength of selection and improved the power of our previous methods to detect partial soft sweeps. Overall, SIA uses deep learning to leverage the ARG and thereby provides new insight into how selective sweeps shape genomic diversity.},
	number = {1},
	urldate = {2022-07-17},
	journal = {Molecular Biology and Evolution},
	author = {Hejase, Hussein A and Mo, Ziyi and Campagna, Leonardo and Siepel, Adam},
	month = jan,
	year = {2022},
	pages = {msab332},
}

@article{torada_imagene_2019,
	title = {{ImaGene}: a convolutional neural network to quantify natural selection from genomic data},
	volume = {20},
	issn = {1471-2105},
	shorttitle = {{ImaGene}},
	url = {https://doi.org/10.1186/s12859-019-2927-x},
	doi = {10.1186/s12859-019-2927-x},
	abstract = {The genetic bases of many complex phenotypes are still largely unknown, mostly due to the polygenic nature of the traits and the small effect of each associated mutation. An alternative approach to classic association studies to determining such genetic bases is an evolutionary framework. As sites targeted by natural selection are likely to harbor important functionalities for the carrier, the identification of selection signatures in the genome has the potential to unveil the genetic mechanisms underpinning human phenotypes. Popular methods of detecting such signals rely on compressing genomic information into summary statistics, resulting in the loss of information. Furthermore, few methods are able to quantify the strength of selection. Here we explored the use of deep learning in evolutionary biology and implemented a program, called ImaGene, to apply convolutional neural networks on population genomic data for the detection and quantification of natural selection.},
	language = {en},
	number = {9},
	urldate = {2022-07-17},
	journal = {BMC Bioinformatics},
	author = {Torada, Luis and Lorenzon, Lucrezia and Beddis, Alice and Isildak, Ulas and Pattini, Linda and Mathieson, Sara and Fumagalli, Matteo},
	month = nov,
	year = {2019},
	keywords = {Convolutional neural networks, Natural selection, Population genetics, Supervised machine learning},
	pages = {337},
}

@article{flagel_unreasonable_2019,
	title = {The {Unreasonable} {Effectiveness} of {Convolutional} {Neural} {Networks} in {Population} {Genetic} {Inference}},
	volume = {36},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msy224},
	doi = {10.1093/molbev/msy224},
	abstract = {Population-scale genomic data sets have given researchers incredible amounts of information from which to infer evolutionary histories. Concomitant with this flood of data, theoretical and methodological advances have sought to extract information from genomic sequences to infer demographic events such as population size changes and gene flow among closely related populations/species, construct recombination maps, and uncover loci underlying recent adaptation. To date, most methods make use of only one or a few summaries of the input sequences and therefore ignore potentially useful information encoded in the data. The most sophisticated of these approaches involve likelihood calculations, which require theoretical advances for each new problem, and often focus on a single aspect of the data (e.g., only allele frequency information) in the interest of mathematical and computational tractability. Directly interrogating the entirety of the input sequence data in a likelihood-free manner would thus offer a fruitful alternative. Here, we accomplish this by representing DNA sequence alignments as images and using a class of deep learning methods called convolutional neural networks (CNNs) to make population genetic inferences from these images. We apply CNNs to a number of evolutionary questions and find that they frequently match or exceed the accuracy of current methods. Importantly, we show that CNNs perform accurate evolutionary model selection and parameter estimation, even on problems that have not received detailed theoretical treatments. Thus, when applied to population genetic alignments, CNNs are capable of outperforming expert-derived statistical methods and offer a new path forward in cases where no likelihood approach exists.},
	number = {2},
	urldate = {2022-07-17},
	journal = {Molecular Biology and Evolution},
	author = {Flagel, Lex and Brandvain, Yaniv and Schrider, Daniel R},
	month = feb,
	year = {2019},
	pages = {220--238},
}

@article{kern_diploshic_2018,
	title = {{diploS}/{HIC}: {An} {Updated} {Approach} to {Classifying} {Selective} {Sweeps}},
	volume = {8},
	issn = {2160-1836},
	shorttitle = {{diploS}/{HIC}},
	url = {https://doi.org/10.1534/g3.118.200262},
	doi = {10.1534/g3.118.200262},
	abstract = {Identifying selective sweeps in populations that have complex demographic histories remains a difficult problem in population genetics. We previously introduced a supervised machine learning approach, S/HIC, for finding both hard and soft selective sweeps in genomes on the basis of patterns of genetic variation surrounding a window of the genome. While S/HIC was shown to be both powerful and precise, the utility of S/HIC was limited by the use of phased genomic data as input. In this report we describe a deep learning variant of our method, diploS/HIC, that uses unphased genotypes to accurately classify genomic windows. diploS/HIC is shown to be quite powerful even at moderate to small sample sizes.},
	number = {6},
	urldate = {2022-07-17},
	journal = {G3 Genes{\textbar}Genomes{\textbar}Genetics},
	author = {Kern, Andrew D and Schrider, Daniel R},
	month = jun,
	year = {2018},
	pages = {1959--1970},
}

@article{adrion_predicting_2020,
	title = {Predicting the {Landscape} of {Recombination} {Using} {Deep} {Learning}},
	volume = {37},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msaa038},
	doi = {10.1093/molbev/msaa038},
	abstract = {Accurately inferring the genome-wide landscape of recombination rates in natural populations is a central aim in genomics, as patterns of linkage influence everything from genetic mapping to understanding evolutionary history. Here, we describe recombination landscape estimation using recurrent neural networks (ReLERNN), a deep learning method for estimating a genome-wide recombination map that is accurate even with small numbers of pooled or individually sequenced genomes. Rather than use summaries of linkage disequilibrium as its input, ReLERNN takes columns from a genotype alignment, which are then modeled as a sequence across the genome using a recurrent neural network. We demonstrate that ReLERNN improves accuracy and reduces bias relative to existing methods and maintains high accuracy in the face of demographic model misspecification, missing genotype calls, and genome inaccessibility. We apply ReLERNN to natural populations of African Drosophila melanogaster and show that genome-wide recombination landscapes, although largely correlated among populations, exhibit important population-specific differences. Lastly, we connect the inferred patterns of recombination with the frequencies of major inversions segregating in natural Drosophila populations.},
	number = {6},
	urldate = {2022-07-17},
	journal = {Molecular Biology and Evolution},
	author = {Adrion, Jeffrey R and Galloway, Jared G and Kern, Andrew D},
	month = jun,
	year = {2020},
	pages = {1790--1808},
}

@article{sheehan_deep_2016,
	title = {Deep {Learning} for {Population} {Genetic} {Inference}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004845},
	doi = {10.1371/journal.pcbi.1004845},
	abstract = {Given genomic variation data from multiple individuals, computing the likelihood of complex population genetic models is often infeasible. To circumvent this problem, we introduce a novel likelihood-free inference framework by applying deep learning, a powerful modern technique in machine learning. Deep learning makes use of multilayer neural networks to learn a feature-based function from the input (e.g., hundreds of correlated summary statistics of data) to the output (e.g., population genetic parameters of interest). We demonstrate that deep learning can be effectively employed for population genetic inference and learning informative features of data. As a concrete application, we focus on the challenging problem of jointly inferring natural selection and demography (in the form of a population size change history). Our method is able to separate the global nature of demography from the local nature of selection, without sequential steps for these two factors. Studying demography and selection jointly is motivated by Drosophila, where pervasive selection confounds demographic analysis. We apply our method to 197 African Drosophila melanogaster genomes from Zambia to infer both their overall demography, and regions of their genome under selection. We find many regions of the genome that have experienced hard sweeps, and fewer under selection on standing variation (soft sweep) or balancing selection. Interestingly, we find that soft sweeps and balancing selection occur more frequently closer to the centromere of each chromosome. In addition, our demographic inference suggests that previously estimated bottlenecks for African Drosophila melanogaster are too extreme.},
	language = {en},
	number = {3},
	urldate = {2022-07-17},
	journal = {PLOS Computational Biology},
	author = {Sheehan, Sara and Song, Yun S.},
	month = mar,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Drosophila melanogaster, Effective population size, Genomics statistics, Invertebrate genomics, Natural selection, Population genetics, Population size},
	pages = {e1004845},
}

@article{schrider_supervised_2018,
	title = {Supervised {Machine} {Learning} for {Population} {Genetics}: {A} {New} {Paradigm}},
	volume = {34},
	issn = {0168-9525},
	shorttitle = {Supervised {Machine} {Learning} for {Population} {Genetics}},
	url = {https://www.sciencedirect.com/science/article/pii/S0168952517302251},
	doi = {10.1016/j.tig.2017.12.005},
	abstract = {As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.},
	language = {en},
	number = {4},
	urldate = {2022-07-17},
	journal = {Trends in Genetics},
	author = {Schrider, Daniel R. and Kern, Andrew D.},
	month = apr,
	year = {2018},
	pages = {301--312},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-07-17},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@article{sudlow_uk_2015,
	title = {{UK} {Biobank}: {An} {Open} {Access} {Resource} for {Identifying} the {Causes} of a {Wide} {Range} of {Complex} {Diseases} of {Middle} and {Old} {Age}},
	volume = {12},
	issn = {1549-1676},
	shorttitle = {{UK} {Biobank}},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001779},
	doi = {10.1371/journal.pmed.1001779},
	abstract = {Cathie Sudlow and colleagues describe the UK Biobank, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
	language = {en},
	number = {3},
	urldate = {2022-07-17},
	journal = {PLOS Medicine},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins, Rory},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Cohort studies, Global health, Intelligence tests, Magnetic resonance imaging, Prospective studies, Questionnaires, Research ethics, Scientists},
	pages = {e1001779},
}

@article{karczewski_mutational_2020,
	title = {The mutational constraint spectrum quantified from variation in 141,456 humans},
	volume = {581},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2308-7},
	doi = {10.1038/s41586-020-2308-7},
	abstract = {Genetic variants that inactivate protein-coding genes are a powerful source of information about the phenotypic consequences of gene disruption: genes that are crucial for the function of an organism will be depleted of such variants in natural populations, whereas non-essential genes will tolerate their accumulation. However, predicted loss-of-function variants are enriched for annotation errors, and tend to be found at extremely low frequencies, so their analysis requires careful variant annotation and very large sample sizes1. Here we describe the aggregation of 125,748 exomes and 15,708 genomes from human sequencing studies into the Genome Aggregation Database (gnomAD). We identify 443,769 high-confidence predicted loss-of-function variants in this cohort after filtering for artefacts caused by sequencing and annotation errors. Using an improved model of human mutation rates, we classify human protein-coding genes along a spectrum that represents tolerance to inactivation, validate this classification using data from model organisms and engineered human cells, and show that it can be used to improve the power of gene discovery for both common and rare diseases.},
	language = {en},
	number = {7809},
	urldate = {2022-07-17},
	journal = {Nature},
	author = {Karczewski, Konrad J. and Francioli, Laurent C. and Tiao, Grace and Cummings, Beryl B. and Alf√∂ldi, Jessica and Wang, Qingbo and Collins, Ryan L. and Laricchia, Kristen M. and Ganna, Andrea and Birnbaum, Daniel P. and Gauthier, Laura D. and Brand, Harrison and Solomonson, Matthew and Watts, Nicholas A. and Rhodes, Daniel and Singer-Berk, Moriel and England, Eleina M. and Seaby, Eleanor G. and Kosmicki, Jack A. and Walters, Raymond K. and Tashman, Katherine and Farjoun, Yossi and Banks, Eric and Poterba, Timothy and Wang, Arcturus and Seed, Cotton and Whiffin, Nicola and Chong, Jessica X. and Samocha, Kaitlin E. and Pierce-Hoffman, Emma and Zappala, Zachary and O‚ÄôDonnell-Luria, Anne H. and Minikel, Eric Vallabh and Weisburd, Ben and Lek, Monkol and Ware, James S. and Vittal, Christopher and Armean, Irina M. and Bergelson, Louis and Cibulskis, Kristian and Connolly, Kristen M. and Covarrubias, Miguel and Donnelly, Stacey and Ferriera, Steven and Gabriel, Stacey and Gentry, Jeff and Gupta, Namrata and Jeandet, Thibault and Kaplan, Diane and Llanwarne, Christopher and Munshi, Ruchi and Novod, Sam and Petrillo, Nikelle and Roazen, David and Ruano-Rubio, Valentin and Saltzman, Andrea and Schleicher, Molly and Soto, Jose and Tibbetts, Kathleen and Tolonen, Charlotte and Wade, Gordon and Talkowski, Michael E. and Neale, Benjamin M. and Daly, Mark J. and MacArthur, Daniel G.},
	month = may,
	year = {2020},
	note = {Number: 7809
Publisher: Nature Publishing Group},
	keywords = {Medical genomics, Rare variants},
	pages = {434--443},
}

@article{auton_global_2015,
	title = {A global reference for human genetic variation},
	volume = {526},
	copyright = {2015 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature15393},
	doi = {10.1038/nature15393},
	abstract = {The 1000 Genomes Project set out to provide a comprehensive description of common human genetic variation by applying whole-genome sequencing to a diverse set of individuals from multiple populations. Here we report completion of the project, having reconstructed the genomes of 2,504 individuals from 26 populations using a combination of low-coverage whole-genome sequencing, deep exome sequencing, and dense microarray genotyping. We characterized a broad spectrum of genetic variation, in total over 88 million variants (84.7 million single nucleotide polymorphisms (SNPs), 3.6 million short insertions/deletions (indels), and 60,000 structural variants), all phased onto high-quality haplotypes. This resource includes {\textgreater}99\% of SNP variants with a frequency of {\textgreater}1\% for a variety of ancestries. We describe the distribution of genetic variation across the global sample, and discuss the implications for common disease studies.},
	language = {en},
	number = {7571},
	urldate = {2022-07-17},
	journal = {Nature},
	author = {Auton, Adam and Abecasis, Gon√ßalo R. and Altshuler, David M. and Durbin, Richard M. and Abecasis, Gon√ßalo R. and Bentley, David R. and Chakravarti, Aravinda and Clark, Andrew G. and Donnelly, Peter and Eichler, Evan E. and Flicek, Paul and Gabriel, Stacey B. and Gibbs, Richard A. and Green, Eric D. and Hurles, Matthew E. and Knoppers, Bartha M. and Korbel, Jan O. and Lander, Eric S. and Lee, Charles and Lehrach, Hans and Mardis, Elaine R. and Marth, Gabor T. and McVean, Gil A. and Nickerson, Deborah A. and Schmidt, Jeanette P. and Sherry, Stephen T. and Wang, Jun and Wilson, Richard K. and Gibbs, Richard A. and Boerwinkle, Eric and Doddapaneni, Harsha and Han, Yi and Korchina, Viktoriya and Kovar, Christie and Lee, Sandra and Muzny, Donna and Reid, Jeffrey G. and Zhu, Yiming and Wang, Jun and Chang, Yuqi and Feng, Qiang and Fang, Xiaodong and Guo, Xiaosen and Jian, Min and Jiang, Hui and Jin, Xin and Lan, Tianming and Li, Guoqing and Li, Jingxiang and Li, Yingrui and Liu, Shengmao and Liu, Xiao and Lu, Yao and Ma, Xuedi and Tang, Meifang and Wang, Bo and Wang, Guangbiao and Wu, Honglong and Wu, Renhua and Xu, Xun and Yin, Ye and Zhang, Dandan and Zhang, Wenwei and Zhao, Jiao and Zhao, Meiru and Zheng, Xiaole and Lander, Eric S. and Altshuler, David M. and Gabriel, Stacey B. and Gupta, Namrata and Gharani, Neda and Toji, Lorraine H. and Gerry, Norman P. and Resch, Alissa M. and Flicek, Paul and Barker, Jonathan and Clarke, Laura and Gil, Laurent and Hunt, Sarah E. and Kelman, Gavin and Kulesha, Eugene and Leinonen, Rasko and McLaren, William M. and Radhakrishnan, Rajesh and Roa, Asier and Smirnov, Dmitriy and Smith, Richard E. and Streeter, Ian and Thormann, Anja and Toneva, Iliana and Vaughan, Brendan and Zheng-Bradley, Xiangqun and Bentley, David R. and Grocock, Russell and Humphray, Sean and James, Terena and Kingsbury, Zoya and Lehrach, Hans and Sudbrak, Ralf and Albrecht, Marcus W. and Amstislavskiy, Vyacheslav S. and Borodina, Tatiana A. and Lienhard, Matthias and Mertes, Florian and Sultan, Marc and Timmermann, Bernd and Yaspo, Marie-Laure and Mardis, Elaine R. and Wilson, Richard K. and Fulton, Lucinda and Fulton, Robert and Sherry, Stephen T. and Ananiev, Victor and Belaia, Zinaida and Beloslyudtsev, Dimitriy and Bouk, Nathan and Chen, Chao and Church, Deanna and Cohen, Robert and Cook, Charles and Garner, John and Hefferon, Timothy and Kimelman, Mikhail and Liu, Chunlei and Lopez, John and Meric, Peter and O‚ÄôSullivan, Chris and Ostapchuk, Yuri and Phan, Lon and Ponomarov, Sergiy and Schneider, Valerie and Shekhtman, Eugene and Sirotkin, Karl and Slotta, Douglas and Zhang, Hua and McVean, Gil A. and Durbin, Richard M. and Balasubramaniam, Senduran and Burton, John and Danecek, Petr and Keane, Thomas M. and Kolb-Kokocinski, Anja and McCarthy, Shane and Stalker, James and Quail, Michael and Schmidt, Jeanette P. and Davies, Christopher J. and Gollub, Jeremy and Webster, Teresa and Wong, Brant and Zhan, Yiping and Auton, Adam and Campbell, Christopher L. and Kong, Yu and Marcketta, Anthony and Gibbs, Richard A. and Yu, Fuli and Antunes, Lilian and Bainbridge, Matthew and Muzny, Donna and Sabo, Aniko and Huang, Zhuoyi and Wang, Jun and Coin, Lachlan J. M. and Fang, Lin and Guo, Xiaosen and Jin, Xin and Li, Guoqing and Li, Qibin and Li, Yingrui and Li, Zhenyu and Lin, Haoxiang and Liu, Binghang and Luo, Ruibang and Shao, Haojing and Xie, Yinlong and Ye, Chen and Yu, Chang and Zhang, Fan and Zheng, Hancheng and Zhu, Hongmei and Alkan, Can and Dal, Elif and Kahveci, Fatma and Marth, Gabor T. and Garrison, Erik P. and Kural, Deniz and Lee, Wan-Ping and Fung Leong, Wen and Stromberg, Michael and Ward, Alistair N. and Wu, Jiantao and Zhang, Mengyao and Daly, Mark J. and DePristo, Mark A. and Handsaker, Robert E. and Altshuler, David M. and Banks, Eric and Bhatia, Gaurav and del Angel, Guillermo and Gabriel, Stacey B. and Genovese, Giulio and Gupta, Namrata and Li, Heng and Kashin, Seva and Lander, Eric S. and McCarroll, Steven A. and Nemesh, James C. and Poplin, Ryan E. and Yoon, Seungtai C. and Lihm, Jayon and Makarov, Vladimir and Clark, Andrew G. and Gottipati, Srikanth and Keinan, Alon and Rodriguez-Flores, Juan L. and Korbel, Jan O. and Rausch, Tobias and Fritz, Markus H. and St√ºtz, Adrian M. and Flicek, Paul and Beal, Kathryn and Clarke, Laura and Datta, Avik and Herrero, Javier and McLaren, William M. and Ritchie, Graham R. S. and Smith, Richard E. and Zerbino, Daniel and Zheng-Bradley, Xiangqun and Sabeti, Pardis C. and Shlyakhter, Ilya and Schaffner, Stephen F. and Vitti, Joseph and Cooper, David N. and Ball, Edward V. and Stenson, Peter D. and Bentley, David R. and Barnes, Bret and Bauer, Markus and Keira Cheetham, R. and Cox, Anthony and Eberle, Michael and Humphray, Sean and Kahn, Scott and Murray, Lisa and Peden, John and Shaw, Richard and Kenny, Eimear E. and Batzer, Mark A. and Konkel, Miriam K. and Walker, Jerilyn A. and MacArthur, Daniel G. and Lek, Monkol and Sudbrak, Ralf and Amstislavskiy, Vyacheslav S. and Herwig, Ralf and Mardis, Elaine R. and Ding, Li and Koboldt, Daniel C. and Larson, David and Ye, Kai and Gravel, Simon and {The 1000 Genomes Project Consortium} and {Corresponding authors} and {Steering committee} and {Production group} and {Baylor College of Medicine} and {BGI-Shenzhen} and {Broad Institute of MIT and Harvard} and {Coriell Institute for Medical Research} and European Molecular Biology Laboratory, European Bioinformatics Institute and {Illumina} and {Max Planck Institute for Molecular Genetics} and {McDonnell Genome Institute at Washington University} and {US National Institutes of Health} and {University of Oxford} and {Wellcome Trust Sanger Institute} and {Analysis group} and {Affymetrix} and {Albert Einstein College of Medicine} and {Bilkent University} and {Boston College} and {Cold Spring Harbor Laboratory} and {Cornell University} and {European Molecular Biology Laboratory} and {Harvard University} and {Human Gene Mutation Database} and {Icahn School of Medicine at Mount Sinai} and {Louisiana State University} and {Massachusetts General Hospital} and {McGill University} and National Eye Institute, NIH},
	month = oct,
	year = {2015},
	note = {Number: 7571
Publisher: Nature Publishing Group},
	keywords = {Genetic variation, Genomics},
	pages = {68--74},
}
